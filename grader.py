"""Process the output generated by nbgrader to be ready for students"""

# pylint:disable=unspecified-encoding,broad-except

import json
import sys
import re
import logging
import os
import shutil
import hashlib
import inspect
from pathlib import Path
from functools import wraps

from nbgrader import __file__ as nbgrader_filename, __version__ as nbgrader_version
from nbgrader.api import MissingEntry
from nbgrader.apps import NbGraderAPI
from nbgrader.converters import Autograde, GenerateFeedback, GenerateAssignment
from nbgrader.preprocessors import NbGraderPreprocessor, Execute
from nbgrader.utils import temp_attrs
from nbconvert.preprocessors import ExecutePreprocessor
from traitlets.traitlets import TraitError
from traitlets.config import Config
from traitlets.config.loader import PyFileConfigLoader, ConfigFileNotFound
import nbformat


# Use the force parameter to override any existing logging config from other libraries.
logging.basicConfig(
    level=logging.DEBUG,
    format="[%(levelname)s] %(message)s",
    stream=sys.stdout,
    force=True,
)

try:
    import grader_hooks

    logging.info("Imported grader hooks from repo.")
except ImportError:
    logging.info("There were no grader hooks to import.")
    logging.debug(
        "Looked in the following paths: %s",
        [str(Path(path).resolve()) for path in sys.path],
    )


# Warnings should never effect a student's grade, so we completely ignore them
# when grading.
os.environ["PYTHONWARNINGS"] = "ignore"


ERROR_MESSAGES = {
    "failed_feedback": (
        "There was a problem in generating the feedback for your grade. This "
        "means your assignment was graded and your score should reflect such."
    ),
    "kernel_unresponsive": (
        "While grading your assignment, the kernel became unresponsive and we "
        "could not interrupt it. This probably means that your code is "
        "allocating too much memory or is not efficient enough."
    ),
    "contact_instructor": (
        "If this feedback is not helpful, please contact your instructor. They "
        "will have access to the full autograder logs which contain more "
        "detail."
    ),
}

CONFIG = Config(
    # Print all the details for the Coursera grader debug output
    NbGraderAPI={"log_level": logging.DEBUG},
    # Make sure we always run the grader
    Autograde={"force": True},
    # If not otherwise specified, default to 5min cell timeout
    ExecutePreprocessor={"timeout": 300},
    GenerateFeedback={
        "force": True,
        "preprocessors": [
            "nbgrader.preprocessors.GetGrades",
            "nbconvert.preprocessors.CSSHTMLHeaderPreprocessor",
            "nbgrader.preprocessors.ClearHiddenTests",
            f"{__name__}.StripHiddenTracebacks",
        ],
        "HTMLExporter": {
            "template_file": "mads-feedback.html.j2",
            # Set the template path to always look next to this file. This isn't a
            # problem in the autograder, but isn't the default for local testing.
            "extra_template_paths": [
                os.path.dirname(__file__),
                os.path.join(
                    os.path.dirname(nbgrader_filename),
                    "server_extensions/formgrader/templates",
                ),
                # By setting extra_template_paths, we're overriding this value:
                # https://github.com/jupyter/nbgrader/blob/main/nbgrader/converters/generate_feedback.py#L65-L66
                os.path.join(
                    os.path.dirname(nbgrader_filename),
                    "server_extensions/formgrader/static/components/bootstrap/css",
                ),
            ],
        },
    },
)

student_name = "test_case_1"
ast_dir = Path("./assets")
rel_dir = Path("./release")
src_dir = Path("./source")
aug_dir = Path("./autograded")
sub_dir = Path(f"./submitted/{student_name}")
out_dir = Path("/shared")


def log_function(func):
    """Log the name of a function before and after it is called."""

    @wraps(func)
    def wrapper(*args, **kwargs):
        disp_args = args
        if isinstance(args[0], Grader):
            disp_args = args[1:]
        logging.debug("----------------------------")
        logging.debug(
            "Calling %s(%s)",
            func.__name__,
            ", ".join("%r" % i for i in disp_args),
        )
        logging.debug("----------------------------")
        result = func(*args, **kwargs)
        logging.debug("----------------------------")
        logging.debug("")
        logging.debug("")
        return result

    return wrapper


def parse_errors(lines):
    """Look for error lines in the autograde log."""

    # Process-level errors. With none of these, there probably wasn't an error.
    error_lines = [line.strip() for line in lines if "ERROR" in line]
    if not error_lines:
        return None

    # Specific cases we know about and have improved messages for.
    if any(line for line in lines if "kernel became unresponsive" in line):
        return ERROR_MESSAGES["kernel_unresponsive"]

    # In-kernel exceptions. We ignore 'except ' to keep out source code, which
    # doesn't help here.
    errors = []
    for line in lines:
        if line.startswith("    ") and "Error:" in line and "except " not in line:
            errors.append(line.strip())

    # If we spot any local errors, use those instead of autograder logs.
    if errors:
        error_lines = errors

    # Make a bulleted list, regardless of where the errors came from.
    for i, line in enumerate(error_lines):
        error_lines[i] = re.sub(r"^(\[ERROR\] )?", "* ", line)

    # Return the list as one formatted string.
    return "There were errors when grading your assignment:\n{}".format(
        "\n".join(error_lines)
    )


class StripHiddenTracebacks(NbGraderPreprocessor):
    """Remove tracebacks from test cells containing hidden tests."""

    CUSTOM_MESSAGES = {
        "KeyboardInterrupt": (
            "This error is python being interrupted. "
            "This is thrown when executing a cell times out."
        )
    }

    def preprocess_cell(self, cell, resources, index):
        nbmeta = cell.get("metadata", {}).get("nbgrader", {})

        # Only process test cells.
        if not nbmeta.get("grade", False):
            return cell, resources

        for output in cell.get("outputs", []):
            if output["output_type"] == "error":
                line = output["traceback"][-1]

                # Replace the real error with a custom message if it matches
                for key, value in self.CUSTOM_MESSAGES.items():
                    if key in line:
                        line = f"{key}: {value}"
                        break

                output["traceback"] = [
                    (
                        "\033[31mYou have failed this test due to an error. "
                        "The traceback has been removed because it may contain hidden tests. "
                        "This is the exception that was thrown:"
                    ),
                    "",
                    line,
                ]

        return cell, resources


@log_function
def print_tree(path):
    """Print a tree structure of the children of a given path."""
    for root, _dirs, files in os.walk(path):
        if "cache" in root:
            continue
        bits = root.split("/")
        indent = " " * 4 * (len(bits) - 1)
        logging.info("%s%s/", indent, bits[-1])
        for file in files:
            sig = hashlib.blake2b(
                Path(root, file).read_bytes(), digest_size=4
            ).hexdigest()
            logging.info(" ".join([indent + (" " * 4), sig, file]))


class Grader:
    """Run nbgrader and generate an output file for use in our autograders."""

    def __init__(self, coursera_dir="/shared", config=None, skip_setup=False):
        """
        Create an nbgrader wrapper.

        Parameters:
        coursera_dir (string): The output directory to write coursera feedback to.
        config (Config): A config object to apply to the entire grader wrapper.
        skip_setup (bool): Should we skip the step of creating directories.
        """

        instance_config = CONFIG.copy()

        if config:
            instance_config.merge(config)

        # Load in the nbgrader config file.
        try:
            loader = PyFileConfigLoader("nbgrader_config.py")
            loaded_config = loader.load_config()
            print(f"Loaded nbgrader config: {loaded_config}")
            instance_config.merge(loaded_config)
            logging.info("Updated config with nbgrader_config.py")
        except ConfigFileNotFound:
            logging.info("No config file nbgrader_config.py found")

        self.nbg = NbGraderAPI(config=instance_config)
        self.skip_setup = skip_setup
        self.coursera_dir = coursera_dir
        self.student = None
        self.assignment = None
        self.notebook = None

    def grade(self, assignment: str, student: str, notebook: str) -> bool:
        """Configure the environment before starting, then run the grading workflow."""

        # Save student so we can use in other methods
        self.student = student
        self.assignment = assignment
        self.notebook = notebook

        # Additionally configure the course directory for _this specific grade_:
        # - Don't ignore any files. Any stated ignores were meant for
        #   generating the assignment. Here, we only have what we want from students.
        # - Only look at the notebook we've mentioned. Don't execute more than we say.
        with temp_attrs(self.nbg.coursedir, ignore=[], notebook_id=notebook):
            # 1. Prepare the filesystem for grading
            if not self.skip_setup:
                self.setup()

            # 2. Create the gradebook and populate it with this assignment.
            self.generate_assignment()
            print_tree("..")

            self.run_hook("pre_grade")

            # 3. Autograde the submitted file for this student.
            if not self.autograde():
                return False

            self.run_hook("post_grade")

            # 4. Generate an HTML file with the results of the autograding.
            if not self.generate_feedback():
                return False

            self.run_hook("feedback")

        return True

    @log_function
    def generate_assignment(self):
        """Run the generate_assignment command.
        This is required to create the gradebook and populate it with the assignment."""

        self.nbg.generate_assignment(self.assignment)

    @log_function
    def autograde(self) -> bool:
        """Run the autograde command."""

        result = self.nbg.autograde(self.assignment, self.student)
        if not result["success"]:
            logging.error("Autograding was not successful:")
            logging.error(result["log"])
            logging.error(result["error"])
            process_errors = parse_errors(result["log"].split("\n"))
            self.save_feedback(feedback=process_errors)
            return False

        logging.info("Grading completed with score %s", self.score())
        return True

    @log_function
    def generate_feedback(self) -> bool:
        """Run the generate_feedback command, then copy the result back to
        the output directory."""

        result = self.nbg.generate_feedback(self.assignment, self.student)
        if not result["success"]:
            logging.error("Generating feedback was not successful:")
            logging.error(result["error"])
            self.save_feedback(feedback=ERROR_MESSAGES["failed_feedback"])
            return False

        logging.info("Feedback has been generated successfully")

        # Copy the feedback into the output directory after processing.
        try:
            feedback_file = (
                f"{self.nbg.coursedir.feedback_directory}/"
                f"{self.student}/{self.assignment}/{self.notebook}.html"
            )
            output_file = f"{self.coursera_dir}/htmlFeedback.html"
            logging.info(
                "Copying HTML feedback from %s to %s", feedback_file, output_file
            )
            with open(feedback_file, "r") as infile:
                with open(output_file, "w") as outfile:
                    outfile.write(infile.read())
        except FileNotFoundError:
            logging.error("Failed copying feedback file.")
            self.save_feedback(feedback=ERROR_MESSAGES["failed_feedback"])
            return False

        # At this point, the feedback is in an HTML file.
        self.save_feedback(
            feedback_type="HTML", feedback="View the HTML feedback for details."
        )
        return True

    @log_function
    def run_hook(self, hook_name):
        """Run a hook if it exists."""

        filemap = {
            "pre_grade": lambda: self.file_at_stage("submitted"),
            "post_grade": lambda: self.file_at_stage("autograded"),
            "feedback": lambda: self.file_at_stage("feedback", ext="html"),
        }

        if hook_name not in filemap:
            logging.error("There is no hook defined for %s", hook_name)
            return

        fn_name = hook_name + "_hook"

        # Load the hook
        hook = getattr(globals().get("grader_hooks"), fn_name, None)
        if not hook:
            logging.info("No function `%s` found", fn_name)
            return

        # Load the notebook
        path = filemap[hook_name]()
        os.chmod(path, 511)

        ntbk = None
        try:
            ntbk = nbformat.read(path, 4)
        except (FileNotFoundError, nbformat.reader.NotJSONError):
            logging.info("The file at this stage isn't a notebook")

        # Assemble the parameters we're passing to the hook based on argument names
        args = {}
        arg_names = inspect.getfullargspec(hook).args

        for arg in arg_names:
            if arg in ["nb", "ntbk"]:
                args[arg] = ntbk

            elif arg in ["path", "nb_path"]:
                args[arg] = Path(path)

            elif arg == "assignment":
                args[arg] = self.assignment

            elif arg == "score":
                args[arg] = self.score()

            else:
                logging.warning("Unknown argument: %s", arg)

        try:
            logging.info("Running %s hook with args %s", hook_name, list(args.keys()))
            hook(**args)
            if ntbk:
                nbformat.write(ntbk, path)
            logging.info("Hook %s completed successfully!", hook_name)
        except Exception as err:
            logging.error(
                "An exception occurred while running the `%s` hook: %s",
                hook_name,
                err,
            )

    def file_at_stage(self, stage, ext="ipynb") -> str:
        """Return the relevant file for a given stage."""

        base_dir = getattr(self.nbg.coursedir, stage + "_directory")
        dir = self.nbg.coursedir.format_path(base_dir, self.student, self.assignment)
        return os.path.join(dir, f"{self.notebook}.{ext}")

    def score(self):
        """
        Get the score of the notebook we just graded. We need to do it this way because
        of multipart assignments: if we're only grading half of an assignment, the
        score for the student or the assignment will be half of what it should be.
        """

        with self.nbg.gradebook as gradebook:
            try:
                submission = gradebook.notebook_submissions(
                    self.notebook, self.assignment
                )[0]
                # Double check this is the correct score. It should be.
                assert submission.student.id == self.student
                return submission.score / submission.max_score
            except (IndexError, MissingEntry, ZeroDivisionError):
                return 0.0

    def save_feedback(self, feedback=None, feedback_type=None):
        """
        Write a JSON object to inform Coursera of the grade and feedback.

        Parameters:
        feedback: Text based feedback to be presented to students.
        feedbackType: TXT or HTML suggesting a feedback file type.
        """

        output = {"fractionalScore": self.score()}

        # Add the message about reaching out if we ever write any plaintext feedback.
        if feedback:
            output["feedback"] = feedback + "\n" + ERROR_MESSAGES["contact_instructor"]

        if feedback_type:
            output["feedbackType"] = feedback_type

        # Write the object to the feedback.json file
        try:
            with open(f"{self.coursera_dir}/feedback.json", "w") as outfile:
                json.dump(output, outfile)
                logging.info(
                    "Feedback written to %s: %s",
                    f"{self.coursera_dir}/feedback.json",
                    output,
                )
        except FileNotFoundError:
            logging.error("Failed to write Coursera feedback.")

    def load_feedback(self):
        """Read the written feedback from disk."""

        with open(f"{self.coursera_dir}/feedback.json", "r") as infile:
            return json.load(infile)

    def report_config(self):
        """Print out the configuration options for various components we use in grading
        in the style that matches Coursera's config output."""

        logging.debug("")
        logging.debug("")
        logging.debug("----------------------------")
        logging.debug("-- NbGrader Config: --------")
        logging.debug("----------------------------")
        logging.debug("nbgrader.__version__ = %s", nbgrader_version)

        instances = [
            #
            # Apps
            self.nbg,
            self.nbg.coursedir,
            #
            # Converters
            Autograde(coursedir=self.nbg.coursedir, parent=self.nbg),
            GenerateFeedback(coursedir=self.nbg.coursedir, parent=self.nbg),
            GenerateAssignment(coursedir=self.nbg.coursedir, parent=self.nbg),
            #
            # Preprocessors
            Execute(parent=self.nbg),
            ExecutePreprocessor(parent=self.nbg),
        ]
        skip = ["config", "parent", "log", "kernel_manager_class"]
        for inst in instances:
            for name in inst.trait_names():
                if name in skip:
                    continue
                try:
                    value = getattr(inst, name)
                    logging.debug(f"c.{inst.__class__.__name__}.{name}={value!r}")
                except TraitError as err:
                    logging.debug(
                        f"TraitError: c.{inst.__class__.__name__}.{name}: {err}"
                    )

        logging.debug("----------------------------")
        logging.debug("")
        logging.debug("")

    @log_function
    def setup(self):
        """Prepare the filesystem to grade a given assignment. This step enforces
        non-standard patterns we have in place for the grading process."""

        # Make sure submitted, autograded, and output directories exist
        sub_dir.mkdir(parents=True, exist_ok=True)
        aug_dir.mkdir(parents=True, exist_ok=True)
        out_dir.mkdir(parents=True, exist_ok=True)

        # Put the default feedback json file into place
        feedback_default = Path("../log/feedback.json")
        if feedback_default.exists():
            shutil.copy(str(feedback_default), str(out_dir / "feedback.json"))

        # Link the assets into place.
        aug_ast_dir = aug_dir / "assets"
        if ast_dir.exists() and not aug_ast_dir.is_symlink():
            logging.debug(
                "Linking from %s -> %s", ast_dir.resolve(), aug_ast_dir.resolve()
            )
            os.symlink(
                str(ast_dir.resolve()),
                str(aug_ast_dir.resolve()),
                target_is_directory=True,
            )


if __name__ == "__main__":
    assignment_name = sys.argv[1]
    notebook_file = sys.argv[2]

    # student_name defaults to "test_case_1". Update if no longer true.
    if len(sys.argv) > 3:
        student_name = sys.argv[3]
        sub_dir = Path(f"./submitted/{student_name}")

    grader = Grader()

    # Report the configuration before we start grading so it's early in the logs
    grader.report_config()

    # Run the whole grading process.
    grader.grade(assignment_name, student_name, notebook_file)
